{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agus/anaconda3/envs/basic/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/agus/anaconda3/envs/basic/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from stanford_postagger.stanford_wrapper import StanfordPOSTagger as StanfordPOSTaggerWrapper\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import scipy\n",
    "from sklearn.grid_search import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('datasets/conll2003/train.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "del lines[0]\n",
    "del lines[0]\n",
    "\n",
    "dataset = []\n",
    "sentence = []\n",
    "for line in lines:\n",
    "    splitter = line.strip().split(' ')\n",
    "    if splitter[0] == '':\n",
    "        continue\n",
    "    elif (splitter[0] == '-DOCSTART-'):\n",
    "        dataset.append(sentence)\n",
    "        sentence = []\n",
    "    else:\n",
    "        token = splitter[0]\n",
    "        tag = splitter[3]\n",
    "        sentence.append((token, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_conlltxt2dataset(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    del lines[0]\n",
    "    del lines[0]\n",
    "    \n",
    "    dataset = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        splitter = line.strip().split(' ')\n",
    "        if splitter[0] == '':\n",
    "            continue\n",
    "        elif (splitter[0] == '-DOCSTART-'):\n",
    "            dataset.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            token = splitter[0]\n",
    "            tag = splitter[3]\n",
    "            sentence.append((token, tag))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = convert_conlltxt2dataset('datasets/conll2003/train.txt')\n",
    "validation_dataset = convert_conlltxt2dataset('datasets/conll2003/valid.txt')\n",
    "test_dataset = convert_conlltxt2dataset('datasets/conll2003/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 'B-ORG'),\n",
       " ('rejects', 'O'),\n",
       " ('German', 'B-MISC'),\n",
       " ('call', 'O'),\n",
       " ('to', 'O')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Postag to Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('+44', 'CD'), ('171', 'CD')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postagger = StanfordPOSTaggerWrapper()\n",
    "postag = postagger.tag('+44 171')\n",
    "postag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_postag2dataset(dataset):\n",
    "    postagger = StanfordPOSTaggerWrapper()\n",
    "    dataset_with_postag = []\n",
    "    for sent in dataset:\n",
    "        postagged_sent = []\n",
    "        for index, (token, tag) in enumerate(sent):\n",
    "            postagged_token = postagger.tag(token)\n",
    "            postagged_sent.append((token, postagged_token[0][1], tag))\n",
    "        dataset_with_postag.append(postagged_sent)\n",
    "        \n",
    "    return dataset_with_postag\n",
    "\n",
    "postagged_train_dataset = add_postag2dataset(train_dataset)\n",
    "postagged_validation_dataset = add_postag2dataset(validation_dataset)\n",
    "postagged_test_dataset = add_postag2dataset(test_dataset)\n",
    "\n",
    "del train_dataset\n",
    "del validation_dataset\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 'NNP', 'B-ORG'),\n",
       " ('rejects', 'VBZ', 'O'),\n",
       " ('German', 'JJ', 'B-MISC'),\n",
       " ('call', 'NN', 'O'),\n",
       " ('to', 'TO', 'O')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postagged_train_dataset[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i, word_check, ortographic_check, ngram_check, postag_check, position_check, bow_check):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = {}\n",
    "    # Ortographic Feature, Word, POSTag & N-Gram\n",
    "    if word_check == True:\n",
    "        features.update({\n",
    "            'word': word,\n",
    "            'word.lower()': word.lower()\n",
    "        })\n",
    "    \n",
    "    if ortographic_check == True:\n",
    "        features.update({\n",
    "            'word.istitle()': word.istitle(),\n",
    "            'word.isdigit()': word.isdigit(),\n",
    "            'word.isupper()': word.isupper()\n",
    "        })\n",
    "    \n",
    "    if ngram_check == True:\n",
    "        features.update({\n",
    "            'word[-3:]': word[-3:],\n",
    "            'word[-2:]': word[-2:],\n",
    "            'word[:2]': word[:2],\n",
    "            'word[:3]': word[:3]\n",
    "        })\n",
    "        \n",
    "    if postag_check == True:\n",
    "        features.update({\n",
    "            'postag': postag\n",
    "        })\n",
    "    \n",
    "    if postag_check == True and ngram_check == True:\n",
    "        features.update({\n",
    "            'postag[:2]': postag[:2]\n",
    "        })\n",
    "    \n",
    "    if position_check == True:\n",
    "        # Position\n",
    "        features.update({\n",
    "            'pos_front': i,\n",
    "            'pos_end': len(sent) - i\n",
    "        })\n",
    "    \n",
    "    if bow_check == True:\n",
    "        # Bag Of Words\n",
    "        if i > 0:\n",
    "            word1 = sent[i-1][0]\n",
    "            postag1 = sent[i-1][1]\n",
    "            features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.istitle()': word1.istitle(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:postag': postag1,\n",
    "                '-1:postag[:2]': postag1[:2]\n",
    "            })\n",
    "        else:\n",
    "            features['BOS'] = True\n",
    "\n",
    "        if i < len(sent) - 1:\n",
    "            word1 = sent[i+1][0]\n",
    "            postag1 = sent[i+1][1]\n",
    "            features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.istitle()': word1.istitle(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "                '+1:postag': postag1,\n",
    "                '+1:postag[:2]': postag1[:2]\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent, word, ortographic, ngram, postag, position, bow):\n",
    "    return [word2features(sent, i, word, ortographic, ngram, postag, position, bow) for i in range(len(sent))]\n",
    "\n",
    "def sent2postag(sent):\n",
    "    return [postag for token, postag, label in sent]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'EU',\n",
       " 'word.lower()': 'eu',\n",
       " 'word.istitle()': False,\n",
       " 'word.isdigit()': False,\n",
       " 'word.isupper()': True,\n",
       " 'word[-3:]': 'EU',\n",
       " 'word[-2:]': 'EU',\n",
       " 'word[:2]': 'EU',\n",
       " 'word[:3]': 'EU',\n",
       " 'postag': 'NNP',\n",
       " 'postag[:2]': 'NN',\n",
       " 'pos_front': 0,\n",
       " 'pos_end': 469,\n",
       " 'BOS': True,\n",
       " '+1:word.lower()': 'rejects',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:postag': 'VBZ',\n",
       " '+1:postag[:2]': 'VB'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(postagged_train_dataset[0], True, True, True, True, True, True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(sent, True, True, True, True, True, True) for sent in postagged_train_dataset]\n",
    "y_train = [sent2labels(sent) for sent in postagged_train_dataset]\n",
    "\n",
    "X_val = [sent2features(sent, True, True, True, True, True, True) for sent in postagged_validation_dataset]\n",
    "y_val = [sent2labels(sent) for sent in postagged_validation_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 s, sys: 819 ms, total: 32.8 s\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "\n",
    "y_pred = crf.predict(X_val)\n",
    "result['all'] = metrics.flat_f1_score(y_val, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(sent, False, True, True, True, True, True) for sent in postagged_train_dataset]\n",
    "y_train = [sent2labels(sent) for sent in postagged_train_dataset]\n",
    "\n",
    "X_val = [sent2features(sent, False, True, True, True, True, True) for sent in postagged_validation_dataset]\n",
    "y_val = [sent2labels(sent) for sent in postagged_validation_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 s, sys: 347 ms, total: 29.3 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = crf.predict(X_val)\n",
    "result['delete_word'] = metrics.flat_f1_score(y_val, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Ortographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(sent, True, False, True, True, True, True) for sent in postagged_train_dataset]\n",
    "y_train = [sent2labels(sent) for sent in postagged_train_dataset]\n",
    "\n",
    "X_val = [sent2features(sent, True, False, True, True, True, True) for sent in postagged_validation_dataset]\n",
    "y_val = [sent2labels(sent) for sent in postagged_validation_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 s, sys: 235 ms, total: 32.2 s\n",
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = crf.predict(X_val)\n",
    "result['delete_ortographic'] = metrics.flat_f1_score(y_val, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(sent, True, True, False, True, True, True) for sent in postagged_train_dataset]\n",
    "y_train = [sent2labels(sent) for sent in postagged_train_dataset]\n",
    "\n",
    "X_val = [sent2features(sent, True, True, False, True, True, True) for sent in postagged_validation_dataset]\n",
    "y_val = [sent2labels(sent) for sent in postagged_validation_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.7 s, sys: 123 ms, total: 24.9 s\n",
      "Wall time: 24.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = crf.predict(X_val)\n",
    "result['delete_ngram'] = metrics.flat_f1_score(y_val, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Postag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(sent, True, True, True, False, True, True) for sent in postagged_train_dataset]\n",
    "y_train = [sent2labels(sent) for sent in postagged_train_dataset]\n",
    "\n",
    "X_val = [sent2features(sent, True, True, True, False, True, True) for sent in postagged_validation_dataset]\n",
    "y_val = [sent2labels(sent) for sent in postagged_validation_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.5 s, sys: 126 ms, total: 34.6 s\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = crf.predict(X_val)\n",
    "result['delete_postag'] = metrics.flat_f1_score(y_val, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(sent, True, True, True, True, False, True) for sent in postagged_train_dataset]\n",
    "y_train = [sent2labels(sent) for sent in postagged_train_dataset]\n",
    "\n",
    "X_val = [sent2features(sent, True, True, True, True, False, True) for sent in postagged_validation_dataset]\n",
    "y_val = [sent2labels(sent) for sent in postagged_validation_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.2 s, sys: 83.8 ms, total: 30.2 s\n",
      "Wall time: 30.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = crf.predict(X_val)\n",
    "result['delete_position'] = metrics.flat_f1_score(y_val, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(sent, True, True, True, True, True, False) for sent in postagged_train_dataset]\n",
    "y_train = [sent2labels(sent) for sent in postagged_train_dataset]\n",
    "\n",
    "X_val = [sent2features(sent, True, True, True, True, True, False) for sent in postagged_validation_dataset]\n",
    "y_val = [sent2labels(sent) for sent in postagged_validation_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 s, sys: 44 ms, total: 21.6 s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = crf.predict(X_val)\n",
    "result['delete_bow'] = metrics.flat_f1_score(y_val, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Result and Importance of Each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': 0.8924325828151741,\n",
       " 'delete_word': 0.8824051054269132,\n",
       " 'delete_ortographic': 0.8879887554440393,\n",
       " 'delete_ngram': 0.8890286700522596,\n",
       " 'delete_postag': 0.8912385871879948,\n",
       " 'delete_position': 0.8915977529617232,\n",
       " 'delete_bow': 0.8659510361894482}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 0.0\n",
      "delete_word -0.01002747738826093\n",
      "delete_ortographic -0.004443827371134801\n",
      "delete_ngram -0.0034039127629145183\n",
      "delete_postag -0.0011939956271793672\n",
      "delete_position -0.000834829853450958\n",
      "delete_bow -0.026481546625725882\n"
     ]
    }
   ],
   "source": [
    "for key in result:\n",
    "    print(key, result[key] - result['all'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

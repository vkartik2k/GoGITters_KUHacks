{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agus/anaconda3/envs/basic/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/agus/anaconda3/envs/basic/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from stanford_postagger.stanford_wrapper import StanfordPOSTagger as StanfordPOSTaggerWrapper\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import scipy\n",
    "from sklearn.grid_search import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('datasets/conll2003/train.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "del lines[0]\n",
    "del lines[0]\n",
    "\n",
    "dataset = []\n",
    "sentence = []\n",
    "for line in lines:\n",
    "    splitter = line.strip().split(' ')\n",
    "    if splitter[0] == '':\n",
    "        continue\n",
    "    elif (splitter[0] == '-DOCSTART-'):\n",
    "        dataset.append(sentence)\n",
    "        sentence = []\n",
    "    else:\n",
    "        token = splitter[0]\n",
    "        tag = splitter[3]\n",
    "        sentence.append((token, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_conlltxt2dataset(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    del lines[0]\n",
    "    del lines[0]\n",
    "    \n",
    "    dataset = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        splitter = line.strip().split(' ')\n",
    "        if splitter[0] == '':\n",
    "            continue\n",
    "        elif (splitter[0] == '-DOCSTART-'):\n",
    "            dataset.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            token = splitter[0]\n",
    "            tag = splitter[3]\n",
    "            sentence.append((token, tag))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = convert_conlltxt2dataset('datasets/conll2003/train.txt')\n",
    "validation_dataset = convert_conlltxt2dataset('datasets/conll2003/valid.txt')\n",
    "test_dataset = convert_conlltxt2dataset('datasets/conll2003/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 'B-ORG'),\n",
       " ('rejects', 'O'),\n",
       " ('German', 'B-MISC'),\n",
       " ('call', 'O'),\n",
       " ('to', 'O')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Postag to Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('+44', 'CD'), ('171', 'CD')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postagger = StanfordPOSTaggerWrapper()\n",
    "postag = postagger.tag('+44 171')\n",
    "postag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_postag2dataset(dataset):\n",
    "    postagger = StanfordPOSTaggerWrapper()\n",
    "    dataset_with_postag = []\n",
    "    for sent in dataset:\n",
    "        postagged_sent = []\n",
    "        for index, (token, tag) in enumerate(sent):\n",
    "            postagged_token = postagger.tag(token)\n",
    "            postagged_sent.append((token, postagged_token[0][1], tag))\n",
    "        dataset_with_postag.append(postagged_sent)\n",
    "        \n",
    "    return dataset_with_postag\n",
    "\n",
    "postagged_train_dataset = add_postag2dataset(train_dataset)\n",
    "postagged_validation_dataset = add_postag2dataset(validation_dataset)\n",
    "postagged_test_dataset = add_postag2dataset(test_dataset)\n",
    "\n",
    "# Delete Unused Dataset\n",
    "del train_dataset\n",
    "del validation_dataset\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 'NNP', 'B-ORG'),\n",
       " ('rejects', 'VBZ', 'O'),\n",
       " ('German', 'JJ', 'B-MISC'),\n",
       " ('call', 'NN', 'O'),\n",
       " ('to', 'TO', 'O')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postagged_train_dataset[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    # Ortographic Feature, Word, POSTag & N-Gram\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word[:2]': word[:2],\n",
    "        'word[:3]': word[:3],\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2]\n",
    "    }\n",
    "    \n",
    "    # Position\n",
    "    features.update({\n",
    "        'pos_front': i,\n",
    "        'pos_end': len(sent) - i\n",
    "    })\n",
    "    \n",
    "    # Bag Of Words\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2postag(sent):\n",
    "    return [postag for token, postag, label in sent]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'EU',\n",
       " 'word.lower()': 'eu',\n",
       " 'word[-3:]': 'EU',\n",
       " 'word[-2:]': 'EU',\n",
       " 'word[:2]': 'EU',\n",
       " 'word[:3]': 'EU',\n",
       " 'word.istitle()': False,\n",
       " 'word.isdigit()': False,\n",
       " 'word.isupper()': True,\n",
       " 'postag': 'NNP',\n",
       " 'postag[:2]': 'NN',\n",
       " 'pos_front': 0,\n",
       " 'pos_end': 469,\n",
       " 'BOS': True,\n",
       " '+1:word.lower()': 'rejects',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:postag': 'VBZ',\n",
       " '+1:postag[:2]': 'VB'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(postagged_train_dataset[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(sent) for sent in postagged_train_dataset]\n",
    "y_train = [sent2labels(sent) for sent in postagged_train_dataset]\n",
    "\n",
    "X_val = [sent2features(sent) for sent in postagged_validation_dataset]\n",
    "y_val = [sent2labels(sent) for sent in postagged_validation_dataset]\n",
    "\n",
    "X_test = [sent2features(sent) for sent in postagged_test_dataset]\n",
    "y_test = [sent2labels(sent) for sent in postagged_test_dataset]\n",
    "\n",
    "del postagged_train_dataset\n",
    "del postagged_validation_dataset\n",
    "del postagged_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'EU',\n",
       " 'word.lower()': 'eu',\n",
       " 'word[-3:]': 'EU',\n",
       " 'word[-2:]': 'EU',\n",
       " 'word[:2]': 'EU',\n",
       " 'word[:3]': 'EU',\n",
       " 'word.istitle()': False,\n",
       " 'word.isdigit()': False,\n",
       " 'word.isupper()': True,\n",
       " 'postag': 'NNP',\n",
       " 'postag[:2]': 'NN',\n",
       " 'pos_front': 0,\n",
       " 'pos_end': 469,\n",
       " 'BOS': True,\n",
       " '+1:word.lower()': 'rejects',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:postag': 'VBZ',\n",
       " '+1:postag[:2]': 'VB'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word Embedding and Add to The Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embedding_model(filename):\n",
    "    f = open(filename, 'r', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    del lines[0]\n",
    "\n",
    "    embedding_dict = {}\n",
    "    counter = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        arr = line.split(' ')\n",
    "        word = arr[0]\n",
    "        vectors = [float(x) for x in arr[1:]]\n",
    "        embedding_dict[word] = vectors\n",
    "    return embedding_dict\n",
    "\n",
    "def add_word_embedding2feature(embedding_dict, feature):\n",
    "    VECTOR_SPACE_SIZE = 50\n",
    "    for i, sentence in enumerate(feature):\n",
    "        for j, token in enumerate(sentence):\n",
    "            word = token['word']\n",
    "            vector = []\n",
    "            if word in embedding_dict:\n",
    "                vector = embedding_dict[word]\n",
    "            else:\n",
    "                vector = [0 for i in range(VECTOR_SPACE_SIZE)]\n",
    "            for k in range(len(vector)):\n",
    "                wk = 'w{}'.format(k+1)\n",
    "                feature[i][j][wk] = vector[k]\n",
    "    return feature\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_embedding_dict = load_word_embedding_model('models/word_embedding/cbow_model_2.txt')\n",
    "X_cbow = deepcopy(X_train)\n",
    "X_cbow = add_word_embedding2feature(cbow_embedding_dict, X_cbow)\n",
    "\n",
    "del cbow_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_embedding_dict = load_word_embedding_model('models/word_embedding/sg_model_2.txt')\n",
    "X_sg = deepcopy(X_train)\n",
    "X_sg = add_word_embedding2feature(sg_embedding_dict, X_sg)\n",
    "\n",
    "del sg_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embedding_dict = load_word_embedding_model('models/word_embedding/fasttext_model_2.txt')\n",
    "X_fasttext = deepcopy(X_train)\n",
    "X_fasttext = add_word_embedding2feature(fasttext_embedding_dict, X_fasttext)\n",
    "\n",
    "del fasttext_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'EU',\n",
       " 'word.lower()': 'eu',\n",
       " 'word[-3:]': 'EU',\n",
       " 'word[-2:]': 'EU',\n",
       " 'word[:2]': 'EU',\n",
       " 'word[:3]': 'EU',\n",
       " 'word.istitle()': False,\n",
       " 'word.isdigit()': False,\n",
       " 'word.isupper()': True,\n",
       " 'postag': 'NNP',\n",
       " 'postag[:2]': 'NN',\n",
       " 'pos_front': 0,\n",
       " 'pos_end': 469,\n",
       " 'BOS': True,\n",
       " '+1:word.lower()': 'rejects',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:postag': 'VBZ',\n",
       " '+1:postag[:2]': 'VB',\n",
       " 'w1': -0.10382087,\n",
       " 'w2': 0.06820139,\n",
       " 'w3': 0.09168298,\n",
       " 'w4': -0.18774457,\n",
       " 'w5': -1.17892,\n",
       " 'w6': -0.19639602,\n",
       " 'w7': 0.11287989,\n",
       " 'w8': 0.4286515,\n",
       " 'w9': 0.13554206,\n",
       " 'w10': -0.16998494,\n",
       " 'w11': -0.6624491,\n",
       " 'w12': -0.2932335,\n",
       " 'w13': -0.19169496,\n",
       " 'w14': -0.217495,\n",
       " 'w15': -0.44583368,\n",
       " 'w16': 0.97934705,\n",
       " 'w17': 0.33488846,\n",
       " 'w18': -0.5221745,\n",
       " 'w19': -0.90460926,\n",
       " 'w20': -0.9685539,\n",
       " 'w21': 1.2006509,\n",
       " 'w22': -0.001496821,\n",
       " 'w23': -0.37980583,\n",
       " 'w24': -1.235236,\n",
       " 'w25': -0.42509484,\n",
       " 'w26': 0.2744287,\n",
       " 'w27': 1.0390763,\n",
       " 'w28': 0.46746588,\n",
       " 'w29': 0.1513747,\n",
       " 'w30': 1.079299,\n",
       " 'w31': 0.6102467,\n",
       " 'w32': -0.4099535,\n",
       " 'w33': 1.0031598,\n",
       " 'w34': -0.073020466,\n",
       " 'w35': 0.4748009,\n",
       " 'w36': -0.34232184,\n",
       " 'w37': -0.27667978,\n",
       " 'w38': -0.815821,\n",
       " 'w39': -0.782363,\n",
       " 'w40': -0.22178179,\n",
       " 'w41': 0.38000762,\n",
       " 'w42': 0.15439937,\n",
       " 'w43': 0.64791316,\n",
       " 'w44': 1.3358219,\n",
       " 'w45': -0.80719304,\n",
       " 'w46': -0.18294902,\n",
       " 'w47': 0.20329438,\n",
       " 'w48': 0.26196295,\n",
       " 'w49': -0.019529425,\n",
       " 'w50': 0.07268766}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cbow[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'EU',\n",
       " 'word.lower()': 'eu',\n",
       " 'word[-3:]': 'EU',\n",
       " 'word[-2:]': 'EU',\n",
       " 'word[:2]': 'EU',\n",
       " 'word[:3]': 'EU',\n",
       " 'word.istitle()': False,\n",
       " 'word.isdigit()': False,\n",
       " 'word.isupper()': True,\n",
       " 'postag': 'NNP',\n",
       " 'postag[:2]': 'NN',\n",
       " 'pos_front': 0,\n",
       " 'pos_end': 469,\n",
       " 'BOS': True,\n",
       " '+1:word.lower()': 'rejects',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:postag': 'VBZ',\n",
       " '+1:postag[:2]': 'VB',\n",
       " 'w1': -0.12734045,\n",
       " 'w2': -0.17149775,\n",
       " 'w3': -0.14360633,\n",
       " 'w4': -0.019517161,\n",
       " 'w5': -0.28408036,\n",
       " 'w6': 0.061121225,\n",
       " 'w7': 0.17245711,\n",
       " 'w8': -0.43180206,\n",
       " 'w9': 0.015727954,\n",
       " 'w10': 0.22075531,\n",
       " 'w11': -0.25180316,\n",
       " 'w12': 0.08385932,\n",
       " 'w13': 0.09272004,\n",
       " 'w14': 0.0015021019,\n",
       " 'w15': 0.012492577,\n",
       " 'w16': 1.1178739,\n",
       " 'w17': -0.3847049,\n",
       " 'w18': -0.1323174,\n",
       " 'w19': -0.22730935,\n",
       " 'w20': -0.5352181,\n",
       " 'w21': 1.4322983,\n",
       " 'w22': 0.43445793,\n",
       " 'w23': -0.20723875,\n",
       " 'w24': -0.6101142,\n",
       " 'w25': -0.3060098,\n",
       " 'w26': -0.4612437,\n",
       " 'w27': 1.0669663,\n",
       " 'w28': 0.09718897,\n",
       " 'w29': 0.1397241,\n",
       " 'w30': 1.2180003,\n",
       " 'w31': 0.73115486,\n",
       " 'w32': -0.91779095,\n",
       " 'w33': -0.6443447,\n",
       " 'w34': 0.07459019,\n",
       " 'w35': 0.567682,\n",
       " 'w36': 0.20993707,\n",
       " 'w37': 0.39971137,\n",
       " 'w38': -0.56781805,\n",
       " 'w39': -0.65492934,\n",
       " 'w40': -0.77791756,\n",
       " 'w41': -0.08138635,\n",
       " 'w42': -0.66018635,\n",
       " 'w43': 0.4727469,\n",
       " 'w44': 1.0510731,\n",
       " 'w45': -0.67745066,\n",
       " 'w46': 0.4013983,\n",
       " 'w47': 0.09936582,\n",
       " 'w48': 0.2254515,\n",
       " 'w49': -0.06519306,\n",
       " 'w50': 0.6091667}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sg[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'EU',\n",
       " 'word.lower()': 'eu',\n",
       " 'word[-3:]': 'EU',\n",
       " 'word[-2:]': 'EU',\n",
       " 'word[:2]': 'EU',\n",
       " 'word[:3]': 'EU',\n",
       " 'word.istitle()': False,\n",
       " 'word.isdigit()': False,\n",
       " 'word.isupper()': True,\n",
       " 'postag': 'NNP',\n",
       " 'postag[:2]': 'NN',\n",
       " 'pos_front': 0,\n",
       " 'pos_end': 469,\n",
       " 'BOS': True,\n",
       " '+1:word.lower()': 'rejects',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:postag': 'VBZ',\n",
       " '+1:postag[:2]': 'VB',\n",
       " 'w1': 0.10213207,\n",
       " 'w2': 0.027069703,\n",
       " 'w3': 0.07607197,\n",
       " 'w4': 0.019441742,\n",
       " 'w5': 0.05452568,\n",
       " 'w6': -0.054628372,\n",
       " 'w7': -0.00013970374,\n",
       " 'w8': -0.19305748,\n",
       " 'w9': 0.044209644,\n",
       " 'w10': 0.045331083,\n",
       " 'w11': -0.004818771,\n",
       " 'w12': 0.09296371,\n",
       " 'w13': 0.05386617,\n",
       " 'w14': 0.018197853,\n",
       " 'w15': -0.0029894128,\n",
       " 'w16': 0.03327747,\n",
       " 'w17': -0.064056106,\n",
       " 'w18': -0.0983447,\n",
       " 'w19': -0.031191973,\n",
       " 'w20': 0.00041868974,\n",
       " 'w21': -0.115362,\n",
       " 'w22': 0.0030208372,\n",
       " 'w23': -0.039958715,\n",
       " 'w24': -0.0072136535,\n",
       " 'w25': -0.03579684,\n",
       " 'w26': 0.00024644737,\n",
       " 'w27': -0.023796747,\n",
       " 'w28': -0.13226575,\n",
       " 'w29': 0.03265484,\n",
       " 'w30': 0.01157206,\n",
       " 'w31': 0.063790694,\n",
       " 'w32': 0.055687,\n",
       " 'w33': -0.04001831,\n",
       " 'w34': -0.023829255,\n",
       " 'w35': 0.11647446,\n",
       " 'w36': 0.0044624126,\n",
       " 'w37': 0.14191934,\n",
       " 'w38': -0.07650271,\n",
       " 'w39': -0.09948927,\n",
       " 'w40': 0.10758327,\n",
       " 'w41': -0.095669866,\n",
       " 'w42': -0.04532902,\n",
       " 'w43': -0.043145612,\n",
       " 'w44': -0.06599398,\n",
       " 'w45': 0.121383056,\n",
       " 'w46': -0.070841625,\n",
       " 'w47': 0.15322621,\n",
       " 'w48': 0.0150953615,\n",
       " 'w49': -0.007020126,\n",
       " 'w50': 0.0002185062}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fasttext[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier Using Best Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_ = 0.001262621084804322\n",
    "c2_ = 0.07748342053200617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.3 s, sys: 1.28 s, total: 1min\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf_cbow = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=c1_,\n",
    "    c2=c1_,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_cbow.fit(X_cbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 586 ms, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf_sg = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=c1_,\n",
    "    c2=c1_,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_sg.fit(X_sg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.7 s, sys: 336 ms, total: 60 s\n",
      "Wall time: 60 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf_fasttext = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=c1_,\n",
    "    c2=c1_,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_fasttext.fit(X_fasttext, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_cbow = 'temp_crf_cbow_2.sav'\n",
    "pickle.dump(crf_cbow, open(filename_cbow, 'wb'))\n",
    "\n",
    "filename_sg = 'temp_crf_sg_2.sav'\n",
    "pickle.dump(crf_sg, open(filename_sg, 'wb'))\n",
    "\n",
    "filename_fasttext = 'temp_crf_fasttext_2.sav'\n",
    "pickle.dump(crf_fasttext, open(filename_fasttext, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
